{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2d4ec7",
   "metadata": {},
   "source": [
    "# Markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7200f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상태전이결과: ['빨강', '빨강', '빨강', '파랑', '빨강', '빨강', '파랑', '빨강', '빨강', '빨강', '빨강']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def markov_prop_simulation(steps):\n",
    "    \n",
    "    # 가능한 상태 정의\n",
    "    states = ['빨강', '파랑', '노랑']\n",
    "    \n",
    "    # 전이확률행렬 정의\n",
    "    transition_matrix={\n",
    "        '빨강' : {'빨강' : 0.5, '파랑':0.3, '노랑':0.2},\n",
    "        '파랑' : {'빨강' : 0.2, '파랑':0.6, '노랑':0.2},\n",
    "        '노랑' : {'빨강' : 0.3, '파랑':0.3, '노랑':0.4},\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # 초기 상태 무작위 선택\n",
    "    current_state = random.choice(states)\n",
    "    sequence = [current_state]\n",
    "    \n",
    "    # 상태 전이 반복 수행\n",
    "    for _ in range(steps):\n",
    "        next_state=random.choices(\n",
    "            population=list(transition_matrix[current_state].keys()), # 무작위 추출 후보군\n",
    "            weights=list(transition_matrix[current_state].values()) # 가중치\n",
    "        )[0]\n",
    "        sequence.append(next_state)\n",
    "        current_state = next_state\n",
    "        \n",
    "        \n",
    "    return sequence\n",
    "\n",
    "simulation_result = markov_prop_simulation(10)\n",
    "print(\"상태전이결과:\", simulation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd31f3e9",
   "metadata": {},
   "source": [
    "# Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a21a947f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7일 후 날씨 예측: ['맑음', '강우', '강우', '맑음', '맑음', '강우', '강우']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "\n",
    "def generate_weather_forecast(days):\n",
    "    \n",
    "    states=[\"맑음\", \"강우\"]\n",
    "    \n",
    "    transition_matrix = {\n",
    "        \"맑음\" : {\"맑음\":0.6, \"강우\":0.4},\n",
    "        \"강우\" : {\"맑음\":0.7, \"강우\":0.3}    \n",
    "    }\n",
    "    \n",
    "    \n",
    "    # 초기 상태를 무작위로 선택\n",
    "    current_state = random.choice(states)\n",
    "    forecast = [current_state]\n",
    "    \n",
    "    for _ in range(days -1):\n",
    "        # 상태 전이 반복 수행\n",
    "        next_state = np.random.choice(\n",
    "            states, p=[transition_matrix[current_state][\"맑음\"], \n",
    "                       transition_matrix[current_state][\"강우\"]]\n",
    "            )\n",
    "        forecast.append(next_state)\n",
    "        current_state=next_state\n",
    "    return forecast\n",
    "\n",
    "whether_forecast=generate_weather_forecast(7)\n",
    "print(\"7일 후 날씨 예측:\", whether_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94e46d",
   "metadata": {},
   "source": [
    "# Markov Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba1fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경로 1 반환값: 1.7500\n",
      "경로 2 반환값: 1.2500\n",
      "경로 3 반환값: 3.2500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# 상태 와 보상 구조 정의\n",
    "\n",
    "states=[\"s\", \"r1\", \"r2\", \"r3\", \"f\"]\n",
    "rewards = {\n",
    "    (\"s\", \"r1\") : 0.5, (\"s\", \"r2\") : 1.5,\n",
    "    (\"r1\", \"r3\") : 1.0, (\"r1\", \"r2\") : 1.5,\n",
    "    (\"r2\", \"r3\") : 2.0, (\"r3\", \"f\") : 3.0 \n",
    "}\n",
    "\n",
    "# 감가율\n",
    "gamma = 1/2\n",
    "\n",
    "# 가능한 경로\n",
    "paths = [\n",
    "    [\"s\", \"r1\", \"r3\", \"f\" ], # path 1\n",
    "    [\"s\", \"r1\", \"r2\", \"f\" ], # path 2\n",
    "    [\"s\", \"r2\", \"r3\", \"f\" ] # path 3\n",
    "] \n",
    "\n",
    "# 반환값 게산 정의 함수\n",
    "def calculate_return(path):\n",
    "    total_return = 0\n",
    "    discount_factor=1 # 초기 감가율\n",
    "    \n",
    "    for i in range(len(path)-1):\n",
    "        state, next_state = path[i], path[i+1]\n",
    "        reward = rewards.get((state, next_state),0) # (state, next_state) 참조, else 0\n",
    "        total_return += discount_factor * reward\n",
    "        discount_factor *= gamma\n",
    "    \n",
    "    return total_return\n",
    "\n",
    "# 모든 경로에 대한 반환값 계산 및 출력\n",
    "returns = {i+1: calculate_return(path) for i, path in enumerate(paths)} # for index, value in enumerate(iterable) 인덱스 항목을 튜플 평태로 반환\n",
    "for path_id, value in returns.items():\n",
    "    print(f\"경로 {path_id} 반환값: {value:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249af0a7",
   "metadata": {},
   "source": [
    "# Markov Decision Process, MDP\n",
    "\n",
    "MDP 는 MRP(markov reward decision)에서 행동과 정책이 추가된 개념. \n",
    "\n",
    "MRP : 시간의 흐름에 따라 상태 전이 확률에 영향을 받으며 자연스럽게 이동\n",
    "MDP : 타임스텝별로 정책에 따라 행동을 선택하고 상태 전이 확률에 영향을 받아 이동\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2da85a",
   "metadata": {},
   "source": [
    "# MDP 상태가치함수 \n",
    "\n",
    " 어떤 상태에서 시작했을 때 앞으로 받을 수 있는 보상의 총합의 기대값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a322781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정책을 따른 기대 반환값(에피소드 평균, 상태가치함수): 2.4287\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# 상태와 행동, 감가율 정의\n",
    "states=[\"s\", \"r1\", \"r2\", \"r3\", \"f\"]\n",
    "actions=[\"a1\", \"a2\"]\n",
    "gamma=0.5\n",
    "\n",
    "# 정책 (s,a) : 상태 s에서 a를 선택할 확률\n",
    "policy = {\n",
    "    \"s\" : {\"a1\" : 0.6, \"a2\":0.4},\n",
    "    \"r1\" : {\"a1\" : 0.7, \"a2\":0.3},\n",
    "    \"r2\" : {\"a1\" : 1},\n",
    "    \"r3\" : {\"a2\" : 1},\n",
    "    \n",
    "}\n",
    "\n",
    "# 상태전이확률 p (s,a,s')\n",
    "transition_probs={\n",
    "    (\"s\", \"a1\") : \"r1\",\n",
    "    (\"s\", \"a2\") : \"r2\",\n",
    "    (\"r1\", \"a1\") : \"r3\",\n",
    "    (\"r1\", \"a2\") : \"r2\",\n",
    "    (\"r2\", \"a1\") : \"r3\",\n",
    "    (\"r3\", \"a2\") : \"f\",\n",
    "}\n",
    "\n",
    "# 보상함수 R(s,a)\n",
    "rewards ={\n",
    "    (\"s\", \"a1\") : 0.5,\n",
    "    (\"s\", \"a2\") : 1.5,\n",
    "    (\"r1\", \"a1\") : 1.0,\n",
    "    (\"r1\", \"a2\") : 1.5,\n",
    "    (\"r2\", \"a1\") : 2.0,\n",
    "    (\"r3\", \"a2\") : 3.0,\n",
    "}\n",
    "\n",
    "# 경로 시뮬레이션 기반 기대 반환값 계산\n",
    "def simulate_episode(start_state=\"s\"):\n",
    "    state = start_state\n",
    "    total_return=0\n",
    "    discount=1.0\n",
    "    \n",
    "    # 에피소드 반복 실행\n",
    "    while state !=\"f\":\n",
    "        # 현재 상태에서 정책에 따라 행동 선택\n",
    "        action_prob = policy[state]\n",
    "        action_list = list(action_prob.keys()) # action_prob 후보군\n",
    "        prob = list(action_prob.values())\n",
    "        action = np.random.choice(action_list, p=prob)\n",
    "        \n",
    "        # 보상 받기\n",
    "        reward = rewards.get((state, action),0)\n",
    "        total_return += discount*reward\n",
    "        \n",
    "        #다음 상태로 이동\n",
    "        next_state = transition_probs.get((state,action), \"f\")\n",
    "        state = next_state\n",
    "        discount*=gamma\n",
    "        \n",
    "    return total_return\n",
    "\n",
    "\n",
    "# 여러번 실행\n",
    "n_episode=10000\n",
    "returns=[]\n",
    "\n",
    "for i in range(n_episode):\n",
    "    episode_return=simulate_episode()\n",
    "    returns.append(episode_return)\n",
    "expected_return = np.mean(returns)\n",
    "\n",
    "print(f\"정책을 따른 기대 반환값(에피소드 평균, 상태가치함수): {expected_return:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
